{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install requests[security]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -q -r ppdiffusers/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDIM 与 DDPM 验证"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"ppdiffusers\")\n",
    "# sys.path.append(\"ppdiffusers/ppdiffusers\")\n",
    "\n",
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "from ppdiffusers import DDPMPipeline, DDPMScheduler, DDIMScheduler\n",
    "\n",
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DDPM 生成图片\n",
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "\n",
    "paddle.seed(33)\n",
    "ddpm_output = pipe()  # 原始 ddpm 输出\n",
    "\n",
    "# 我们采用 DDPM 的训练结果，通过 DDIM Scheduler 来进行采样。\n",
    "pipe.scheduler = DDIMScheduler()\n",
    "\n",
    "# 设置与 DDPM 相同的采样结果，令 DDIM 采样过程中的 eta = 1.\n",
    "paddle.seed(33)\n",
    "ddim_output = pipe(num_inference_steps=1000, eta=1)\n",
    "\n",
    "imgs = [ddpm_output.images[0], ddim_output.images[0]]\n",
    "titles = [\"ddpm\", \"ddim\"]\n",
    "compare_imgs(imgs, titles)  # 该函数在 notebook_utils.py 声明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算机浮点数精度判断"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 获得 DDPM, DDIM 采样器\n",
    "ddpmscheduler = DDPMScheduler()\n",
    "ddimscheduler= DDIMScheduler()\n",
    "ddimscheduler.set_timesteps(1000)\n",
    "ddpmscheduler.set_timesteps(1000)\n",
    "\n",
    "print(\"ddim get variance for step 999\", ddimscheduler._get_variance(999,998))\n",
    "print(\"ddpm get variance for step 999\", ddpmscheduler._get_variance(999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去clip操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPM加速采样"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "pipe.progress_bar = lambda x:x  # uncomment to see progress bar\n",
    "\n",
    "# 我们采用 DDPM 的训练结果，通过 DDIM Scheduler 来进行采样。\n",
    "# print(\"Default setting for DDPM:\\t\",pipe.scheduler.config.clip_sample)  # True\n",
    "pipe.scheduler.config.clip_sample = False\n",
    "paddle.seed(33)\n",
    "ddpm_output = pipe()\n",
    "\n",
    "pipe.scheduler = DDIMScheduler()\n",
    "# print(\"Default setting for DDIM:\\t\",pipe.scheduler.config.clip_sample)  # True\n",
    "pipe.scheduler.config.clip_sample = False\n",
    "paddle.seed(33)\n",
    "ddim_output = pipe(num_inference_steps=1000, eta=1)\n",
    "\n",
    "imgs = [ddpm_output.images[0], ddim_output.images[0]]\n",
    "titles = [\"DDPM no clip\", \"DDIM no clip\"]\n",
    "compare_imgs(imgs, titles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "pipe.progress_bar = lambda x:x  # cancel process bar\n",
    "etas = [0, 0.4, 0.8]\n",
    "steps = [10, 50, 100, 1000]\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "for i in range(len(etas)):\n",
    "    for j in range(len(steps)):\n",
    "        plt.subplot(len(etas), len(steps), j+i*len(steps) + 1)\n",
    "        paddle.seed(77)\n",
    "        sample1 = pipe(num_inference_steps=steps[j], eta=etas[i])\n",
    "        plt.imshow(sample1.images[0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"eta {etas[i]}|step {steps[j]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#图片DDIM采样确定性\n",
    "paddle.seed(77)\n",
    "x_t = paddle.randn((1, 3, 256, 256))\n",
    "paddle.seed(8)\n",
    "sample1 = pipe(num_inference_steps=50,eta=0,x_t=x_t)\n",
    "paddle.seed(9)\n",
    "sample2 = pipe(num_inference_steps=50,eta=0,x_t=x_t)\n",
    "compare_imgs([sample1.images[0], sample2.images[0]], [\"sample(seed 8)\", \"sample(seed 9)\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#图片重建\n",
    "from PIL import Image\n",
    "# 查看原始图片\n",
    "raw_image = Image.open(\"imgs/sample2.png\").crop((0,0,350,350)).resize((256,256))\n",
    "raw_image.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def reverse_sample(self, model_output, x, t, prev_timestep):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model and x_t using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "\n",
    "        alpha_bar_t_next = self.alphas_cumprod[t]\n",
    "        alpha_bar_t = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
    "\n",
    "        inter = (\n",
    "                        ((1-alpha_bar_t_next)/alpha_bar_t_next)** (0.5)- \\\n",
    "                        ((1-alpha_bar_t)/alpha_bar_t)** (0.5)\n",
    "                    )\n",
    "        x_t_next = alpha_bar_t_next** (0.5) * (x/ (alpha_bar_t ** (0.5)) + \\\n",
    "                    (\n",
    "                    model_output * inter\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return x_t_next"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 进行反向采样与解码\n",
    "T = 200\n",
    "\n",
    "def add_noise_by_reverse_sample(pipe, raw_image, T):\n",
    "    \"\"\"\n",
    "    receive a raw image, convert to $x_0$ and construct $x_{t}$ using reverse sample.\n",
    "    \"\"\"\n",
    "    image = paddle.to_tensor([np.array(raw_image)])\n",
    "    image = (image/127.5 - 1).transpose([0,3,1,2])\n",
    "    pipe.scheduler.set_timesteps(T)\n",
    "    with paddle.no_grad():\n",
    "        for t in pipe.progress_bar(pipe.scheduler.timesteps[::-1]):\n",
    "            prev_timestep = t - pipe.scheduler.config.num_train_timesteps // pipe.scheduler.num_inference_steps\n",
    "            model_output=pipe.unet(image, prev_timestep).sample\n",
    "            image = pipe.scheduler.reverse_sample(model_output=model_output,\n",
    "                                                  x=image,\n",
    "                                                  t=t,\n",
    "                                                  prev_timestep=prev_timestep)\n",
    "    image2show = (image / 2 + 0.5).clip(0, 1).transpose([0, 2, 3, 1]).cast(\"float32\").numpy()\n",
    "    image2show = pipe.numpy_to_pil(image2show)\n",
    "    return image, image2show\n",
    "pipe.scheduler.config.clip_sample = False  # 同上述实验，我们必须关掉 clip\n",
    "\n",
    "image, image2show = add_noise_by_reverse_sample(pipe, raw_image, T)\n",
    "sample1 = pipe(num_inference_steps=T,eta=0,x_t=image)\n",
    "\n",
    "# see what image look like\n",
    "compare_imgs([sample1.images[0],image2show[0]], [f\"Reconstructed Image (T={T})\",f\"Reversed Noise(T={T})\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reference: https://github.com/ermongroup/ddim/blob/main/runners/diffusion.py#L296\n",
    "def slerp(z1, z2, alpha):\n",
    "    theta = paddle.acos(paddle.sum(z1 * z2) / (paddle.norm(z1) * paddle.norm(z2)))\n",
    "    return (\n",
    "        paddle.sin((1 - alpha) * theta) / paddle.sin(theta) * z1\n",
    "        + paddle.sin(alpha * theta) / paddle.sin(theta) * z2\n",
    "    )\n",
    "alphas = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "img_size = 1\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "for i in range(len(alphas)):\n",
    "    x_t = slerp(z_0, z_1, alphas[i])\n",
    "    sample_merge = pipe(num_inference_steps=50,eta=0,x_t=x_t)\n",
    "    plt.subplot(1,len(alphas),1+i)\n",
    "    plt.imshow(sample_merge.images[0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 查看原始图片\n",
    "raw_image_1 = Image.open(\"imgs/sample2.png\").crop((20,20,330,330)).resize((256,256))\n",
    "raw_image_2 = sample1.images[0]\n",
    "\n",
    "compare_imgs([raw_image_1, raw_image_2], [\"image 1\", \"image 2\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 融合两张图片\n",
    "T = 50\n",
    "alpha = 0.81  # alpha 参数很重要\n",
    "z_1, _ = add_noise_by_reverse_sample(pipe, raw_image_1, T)\n",
    "z_2, _ = add_noise_by_reverse_sample(pipe, sample1.images[0], T)\n",
    "x_t = slerp(z_1, z_2, alpha)\n",
    "sample_merge = pipe(num_inference_steps=T,eta=0,x_t=x_t)\n",
    "compare_imgs([sample1.images[0],sample_merge.images[0] ], [\"sample 1\", \"sample 1 merged messi style\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "paddle.seed(77)\n",
    "pipe.scheduler.config.clip_sample = False\n",
    "\n",
    "z_0 = paddle.randn((1, 3, 256, 256))\n",
    "sample1 = pipe(num_inference_steps=50,eta=0,x_t=z_0)\n",
    "paddle.seed(2707)\n",
    "z_1 = paddle.randn((1, 3, 256, 256))\n",
    "sample2 = pipe(num_inference_steps=50,eta=0,x_t=z_1)\n",
    "compare_imgs([sample1.images[0], sample2.images[0]], [\"sample from z_0\", \"sample from z_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T17:49:22.097913Z",
     "start_time": "2023-12-07T17:48:47.476439Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from diffusers import DDIMPipeline, DDPMPipeline\n",
    "\n",
    "model_id = \"google/ddpm-cifar10-32\"\n",
    "\n",
    "# load model and scheduler\n",
    "ddim = DDIMPipeline.from_pretrained(model_id)\n",
    "\n",
    "# run pipeline in inference (sample random noise and denoise)\n",
    "image = ddim(num_inference_steps=500).images[0]\n",
    "\n",
    "# save image\n",
    "image.save(\"ddim_generated_image_500.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T17:47:48.771223Z",
     "start_time": "2023-12-07T17:47:46.940846Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T17:36:38.469829Z",
     "start_time": "2023-12-07T17:36:38.462529Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T17:36:47.061123Z",
     "start_time": "2023-12-07T17:36:47.017329Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dldemos.ddim.ddpm import DDPM\n",
    "\n",
    "\n",
    "class DDIM(DDPM):\n",
    "\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 n_steps: int,\n",
    "                 min_beta: float = 0.0001,\n",
    "                 max_beta: float = 0.02):\n",
    "        super().__init__(device, n_steps, min_beta, max_beta)\n",
    "\n",
    "    def sample_backward(self,\n",
    "                        img_or_shape,\n",
    "                        net,\n",
    "                        device,\n",
    "                        simple_var=True,\n",
    "                        ddim_step=20,\n",
    "                        eta=1):\n",
    "        if simple_var:\n",
    "            eta = 1\n",
    "        ts = torch.linspace(self.n_steps, 0,\n",
    "                            (ddim_step + 1)).to(device).to(torch.long)\n",
    "        if isinstance(img_or_shape, torch.Tensor):\n",
    "            x = img_or_shape\n",
    "        else:\n",
    "            x = torch.randn(img_or_shape).to(device)\n",
    "        batch_size = x.shape[0]\n",
    "        net = net.to(device)\n",
    "        for i in tqdm(range(1, ddim_step + 1),\n",
    "                      f'DDIM sampling with eta {eta} simple_var {simple_var}'):\n",
    "            cur_t = ts[i - 1] - 1\n",
    "            prev_t = ts[i] - 1\n",
    "\n",
    "            ab_cur = self.alpha_bars[cur_t]\n",
    "            ab_prev = self.alpha_bars[prev_t] if prev_t >= 0 else 1\n",
    "\n",
    "            t_tensor = torch.tensor([cur_t] * batch_size,\n",
    "                                    dtype=torch.long).to(device).unsqueeze(1)\n",
    "            eps = net(x, t_tensor)\n",
    "            var = eta * (1 - ab_prev) / (1 - ab_cur) * (1 - ab_cur / ab_prev)\n",
    "            noise = torch.randn_like(x)\n",
    "\n",
    "            first_term = (ab_prev / ab_cur)**0.5 * x\n",
    "            second_term = ((1 - ab_prev - var)**0.5 -\n",
    "                           (ab_prev * (1 - ab_cur) / ab_cur)**0.5) * eps\n",
    "            if simple_var:\n",
    "                third_term = (1 - ab_cur / ab_prev)**0.5 * noise\n",
    "            else:\n",
    "                third_term = var**0.5 * noise\n",
    "            x = first_term + second_term + third_term\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T17:50:07.812946Z",
     "start_time": "2023-12-07T17:49:22.721985Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install diffusers\n",
    "from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\n",
    "\n",
    "model_id = \"google/ddpm-bedroom-256\"\n",
    "\n",
    "# load model and scheduler\n",
    "ddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference\n",
    "\n",
    "# run pipeline in inference (sample random noise and denoise)\n",
    "image = ddpm().images[0]\n",
    "\n",
    "\n",
    "# save image\n",
    "image.save(\"ddpm_generated_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
